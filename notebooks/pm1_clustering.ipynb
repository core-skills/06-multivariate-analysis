{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PM1 - Clustering and regression with correlated high dimension data\n",
    "\n",
    "This noteook has two main exercises:\n",
    "\n",
    "### Exercise 1: K-means clustering in scikit-learn\n",
    "* Apply k-means to find clusters in the synthetic drilling data and the Octane dataset.  If you wish to you can try doing this in the original high dimension dataset, but this will fail.  Instead try using the first few principal components that you've found in the previous example.\n",
    "\n",
    "### Exercise 2: Principal Components Regression\n",
    "* Train a linear regression model (least squares or robust, and using either Seaborn or scikit-learn - try them all if you'd like to!) on the drilling or Octane datasets.  You cannot use the datasets at their full dimension because there are less observations than there are features (in the original feature space)!  But you'll likely find that you train a model based on the PC-transformed features.  Use the code from the last two sessions to do this.  You can start simple - Try to predict quartz based on the first PC, try the same with Octane.  Then build on this as you become more confident and ambitious! \n",
    "\n",
    "Also, see if you can find out how to use scikit-learn pipelines to implement this as a pipeline with a transform, followed by a linear regression.  You may find the [pm1-linear-models](../../05-simple-predictions/notebooks/pm1-linear-models-II_filled.ipynb) useful to review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: K-means clustering in scikit-learn\n",
    "\n",
    "The k-means algorithm searches for clusters in an unlabelled multidimensional (low-dimension) dataset.  If your dataset is high dimensional then you'll need to reduce the dimension, using, for example, principal components.\n",
    "\n",
    "K-means clustering assumes that the centre of a cluster should be the mean average of all of the points belonging to the cluster. The algorithm relies on the assumption that each point should be closer to its own cluster's centre than to any other cluster centre.  This has many of the characteristics (and flaws) of the linear models we've been looking at over the last couple of weeks.  That said, it's often worth trying before moving to more sophisticated approaches.\n",
    "\n",
    "You can also easily find information on how to use k-means in scikit-learn through internet searches, but lets use a textbook example, then try applying this to another dataset.\n",
    "\n",
    "Explore the other objects and methods in sklearn.cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the necessary packages we will be using and creating some example data to work with using the ```make_blobs``` command in scikit-learns datasets module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                      cluster_std=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by applying K-means clustering to the data. The main function we will be using is ```KMeans```\n",
    "\n",
    "```from sklearn.cluster import KMeans```\n",
    "\n",
    "Have a look at the documentation and\n",
    "1. Create a KMeans model instance with 4 clusters\n",
    "2. Fit the model using the explanatoary variables (X)\n",
    "3. Predict the cluster means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  import \n",
    "# 1. Create a KMeans model instance with 4 clusters\n",
    "kmodel = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fit the model using the explanatoary variables (X) we want to cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Predict the cluster means\n",
    "y_kmeans ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at y_kmeans.  It has assigned a cluster number to each observation.\n",
    "plt.scatter(X[:, 0], X[:,1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmodel.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked well!\n",
    "\n",
    "Before we move on let's take a moment and reflect on how easy that was to implement now that we are familiar with the scikit-learn API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring a random state\n",
    "\n",
    "Before we move on, let's take a moment to explore how the initial centroid locations are selected. Create a second `KMeans` model instance with the following parameters:\n",
    "```\n",
    "n_clusters=4, init='random',n_init=1, max_iter=1\n",
    "```\n",
    "Take a look at the docs to see what these extra parameters are controlling. Essentially, we are going to look at the initialization of cluster locations by setting the number of iterations to 1.\n",
    "\n",
    "Copy and paste your code from above into 1 cell below (for ease of running multiple times) that will:\n",
    "\n",
    "1. Create a KMeans model instance with 4 clusters\n",
    "2. Fit the model using the explanatoary variables (X) we want to cluster\n",
    "3. Predict the cluster means\n",
    "4. Plot the data and k-means cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmodel2 = \n",
    "\n",
    "y_kmeans2 = \n",
    "plt.\n",
    "centers2 =\n",
    "plt.\n",
    "centers2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above code block a few times. Do the initial cluster centre locations change? They should.\n",
    "\n",
    "The `init='random'` parameter is telling the algorithm to randomly select the data points to initially use as cluster centers. Change the `init` parameter to `init=k-means++` and rerun the code block a few times. Do the initial cluster centre locations change? They should, but in a less extreme manner. While the `k-means++` is using a smarter way to select the starting locations, there is still some use of a random *seed* to select points used for the calculation.\n",
    "\n",
    "The use of *random* numbers is pretty common, and we have used a random number generator in previous examples though it may not have been obvious. The use of random selection can be useful and helpful at times, however just as in laboratory experiments, the idea of *reproducibility* in our data experiments is important.\n",
    "\n",
    "If we want to share code and be able to produce the same results as a collegue so we have consistent plots or data, we can ensure that our random numbers are consistent by using the same *seed* to tell our random number generators to generate the same random numbers. To do this, *set the seed* to 42 by using the parameter `random_state=42` in the `KMeans` model instance. Rerun the code block a few times. Are the centres the same? They should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, enough with random tangents. Now let's combine PCA and clustering..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: K-means clustering of principal components\n",
    "\n",
    "Now try this with some of the clusters that we observed this morning.\n",
    "\n",
    "Firstly, let's try this with the synthetic drilling data, after transformation with PC1 and PC2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we will use\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from regression_help import create_composition_dataframe, create_observations, create_templates_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer back to the notebooks this morning to generate templates and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the templates as per the morning\n",
    "templates = \n",
    "# Create 150 sets of ground truth assays\n",
    "compositions = \n",
    "# Use the templates and ground truth to create synthetic observations\n",
    "observations = \n",
    "# Transpose them to put observations into tidy data format\n",
    "transposed_observations = observations.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a PCA instance and compute (fit) the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = \n",
    "# Fit the PCA model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the observations to get the 'scores' in the PCA space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull out PC1 and PC2, and put this in our features matrix (X), so that we can apply k-means clustering to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scores[:, [0,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three lines of code should:\n",
    "1. Create a KMeans instance with 3 clusters\n",
    "2. Calculate (fit) the clusters\n",
    "3. Predict the cluster labels using the first two principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_model = \n",
    "\n",
    "cluster_labels = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, plot PC1 and PC2 with the k-means predicted cluster labels and cluster centres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = clustering_model.predict(X)\n",
    "plt.scatter(X[:,0], X[:,1], c=cluster_labels, s=50, cmap='viridis')\n",
    "cluster_centres = clustering_model.cluster_centers_\n",
    "plt.scatter(cluster_centres[:, 0], cluster_centres[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that look? Is it what you expected to see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: K-means clustering of principal components\n",
    "\n",
    "Let's try the same with the Octane data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the octane data like we did this morning.\n",
    "\n",
    "Keep the Octane number in another variable, call it ```octane_assays``` because we are going to use it for regression in the next activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "octane_dataframe ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "octane_assays = octane_dataframe['Octane number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now go ahead and drop these\n",
    "octane_dataframe = \\\n",
    "    octane_dataframe.drop(columns=['Sample name', 'Octane number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next put four lines of code here to:\n",
    "1. Pull the observation values out of the dataframe\n",
    "2. Create a PCA instance (with 25 components)\n",
    "3. Fit the PCA model\n",
    "4. Transform the observations into PCA space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = \n",
    "pca = \n",
    "\n",
    "scores = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next four lines of code should:\n",
    "1. Pull out PC1 and PC2, and put this in our features matrix (X)\n",
    "2. Create a KMeans instance with 2 clusters\n",
    "3. Calculate (fit) the clusters\n",
    "4. Predict the cluster labels using the first two principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \n",
    "clustering_model = \n",
    "\n",
    "cluster_labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=cluster_labels, s=50, cmap='viridis')\n",
    "cluster_centres = clustering_model.cluster_centers_\n",
    "plt.scatter(cluster_centres[:, 0], cluster_centres[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to what we did in the morning, but k-means has semi-automatically found the inlier and outlier groups in Octane.  This is reasonable as is, if we knew beforehand that observations are likely to be in one of two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use the cluster labels to split the observations\n",
    "# into inliers and outliers.  We could use\n",
    "# clustering_model.predict to classify new observations\n",
    "# into the inlier or outlier groups.  If we're making\n",
    "# a regression model then we could choose to report\n",
    "# a gross deviation from normal operation if the\n",
    "# new observation is assigned to the outlier group\n",
    "# and we could make an octane prediction if it fits\n",
    "# into the inlier group.\n",
    "\n",
    "inlier_indices = np.where((cluster_labels == 0))[0]\n",
    "outlier_indices = np.where ((cluster_labels == 1))[0]\n",
    "inliers = observations[inlier_indices]\n",
    "outliers = observations[outlier_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do the inliers look?\n",
    "fig, ax = plt.subplots(1,1)\n",
    "for idx in inlier_indices:\n",
    "    plt.plot(observations[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Principal Components Regression\n",
    "\n",
    "This is something you can fully implement with what you already know.  First find the principal components and create a linear regression model that predicts octane based using the principal component scores as the features.  Plot predicted octane against assay octane and look at the R^2 and the residuals.\n",
    "\n",
    "This is a real industrial application.  There is a an expensive and slow analytical process for measuring the octane rating of petrol, but it's not useful for production, where it's desirable that we have continuous and cheap online monitoring of octane rating as hydrocarbons are blended together.  To achieve this infrared spectroscopy is used and a model is constructed that relates octane rating to the observed spectra.  Once the model is created and validated it is used to predict octane rating continuously and cheaply.\n",
    "\n",
    "By using principal components we can make a predictive model with hundreds of observed variables and only ~30 observations.  This is impossible with linear least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to find a linear model that relates octane rating to the principal\n",
    "# components of the spectra.  We'll take out the outliers first.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function below by importing the Octane dataset using pandas and dropping two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_octane_observations_and_assays(inlier_indices):\n",
    "    octane_dataframe = \n",
    "    octane_assays = octane_dataframe['Octane number'][inlier_indices]\n",
    "\n",
    "    octane_dataframe = # drop some columns\n",
    "    \n",
    "    observations = octane_dataframe.values[inlier_indices]\n",
    "    return observations, octane_assays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function below by:\n",
    "1. Create a PCA model instance with ```number_of_components```\n",
    "2. Fit the PCA model\n",
    "3. Transform the observations into PCA space\n",
    "4. Create a ```LinearRegression``` model instance\n",
    "5. Fit regression model\n",
    "6. Predict using features matrix (i.e PCA scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(number_of_components):\n",
    "    observations, assays = get_octane_observations_and_assays(inlier_indices)\n",
    "    pca =    # 1. PCA model instance \n",
    "             # 2. Fit PCA model\n",
    "    scores = # 3. Transform into PCA space\n",
    "\n",
    "    model =  # 4. LinearRegression model instance\n",
    "    features_matrix = scores[:,:]\n",
    "    target_array = assays\n",
    "             # 5. Fit regression model\n",
    "    print('Model R^2 is ',model.score(features_matrix, target_array))\n",
    "    \n",
    "    predicted_octane = #6. Predict using features\n",
    "    \n",
    "    plt.scatter(assays, predicted_octane)\n",
    "    \n",
    "    plt.xlabel('Octane Rating Assay')\n",
    "    plt.ylabel('Octane Rating Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter 1 specifies that we're building a model using just the first principal component.  This is what we get\n",
    "# if we try to find a univariate linear relationship between the first principal component scores and octane rating.\n",
    "build_model(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on just PC1 there is an approximate linear relationship, but it looks like there is unmodelled non-gaussian error present.  The relationship may be improved by including additional PCs.  Try building the model with the first two PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will the predictions improve if we consider 4 principal components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we include 25 principal components?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use 25 components we have a very tight fit, but we are likely overfitting the model to the training data that we have.  We need a way to choose the optimium number of components to use such that we're not overfitting the model, but we still have good predictive power.  We saw this demonstrated in the logistic regression model last time.  Let's look at it in the context of fitting a general linear model (in this case with the \"parts\" made of polynomial terms) to a function similar to the metal recovery vs sulphur data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation revisited\n",
    "\n",
    "Let's use cross validation to fit a polynomial model to the metal recovery dataset, and determine the degree.  This is analogous to hyperparameter selection that will be used for other scikit-learn models.\n",
    "\n",
    "We will use functions to make coding this more compact. The first is a function to perform the polynomial regression using a scikit-learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                        LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also have a function to generate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_data(N, err=1.0, rseed=1):\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    X = rng.rand(N, 1)**2\n",
    "    y = 10 - 1.0 / (X.ravel()+0.1)\n",
    "    if err > 0:\n",
    "        y+= err * rng.randn(N)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create N=50 data\n",
    "X, y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply polynomial regression via the function above using three different degrees and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()\n",
    "\n",
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "plt.scatter(X.ravel(), y, color = 'black')\n",
    "axis = plt.axis()\n",
    "for degree in [1, 3, 5]:\n",
    "    y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)\n",
    "    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))\n",
    "plt.xlim(-0.1, 1.0)\n",
    "plt.ylim(-2, 12)\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time, we looked at two scikit-learn methods to examine model scores. This time, let's use the ```validation_curve``` in ```sklearn.model_selection``` to generate model scores for a range of polynomial degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "degree = np.arange(0, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score, val_score = validation_curve(PolynomialRegression(), X, y, \n",
    "                                         'polynomialfeatures__degree',\n",
    "                                         degree, cv=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\n",
    "plt.plot(degree, np.median(val_score, 1), color='red', label='validation_score')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0, 1.2)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves\n",
    "\n",
    "The optimal model will usually depend on how much training data you have.  Learning curves are a tool to identify when a model will be improved by more data, or whether it's already about as good as it will get (though a different kind of model may still work better). Let's create more data and compare with the validation curves in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, y2 = make_data(200)\n",
    "plt.scatter(X2.ravel(), y2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = \n",
    "train_score2, val_score2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degree, np.median(train_score2, 1), color='blue',\n",
    "        label='traiing score')\n",
    "plt.plot(degree, np.median(val_score2, 1), color='red', label='validation score')\n",
    "plt.plot(degree, np.median(train_score, 1), color='blue', alpha=0.3,\n",
    "         linestyle='dashed')\n",
    "plt.plot(degree, np.median(val_score, 1), color='red', alpha=0.3,\n",
    "        linestyle='dashed')\n",
    "plt.legend(loc='lower center')\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curves behave like this:\n",
    "\n",
    "* A model of a given complexity will overfit a small dataset: this means the training score will be relatively high, while the validation score will be relatively low.\n",
    "\n",
    "* A model of a given complexity will underfit a large dataset: this means that the training score will decrease, but the validation score will increase.\n",
    "\n",
    "* A model will never, except by chance or a mistake in your code, give a better score to the validation set then the training set: this means the curves should keep getting closer together, but never cross.\n",
    "\n",
    "* This all implies that if a model is being overfitted, then the behaviour may improve if you increase the size of the training set.  This is a reason why neural network models, with their high complexity, require lots of training data to behave well.  Conversely, when the training and validation curves have converged then you don't need to spend time, energy or money collecting more data (unless you intend to bring in a higher complexity model in future).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a cross-validated Principal Component Regression (PCR) model\n",
    "\n",
    "Lets apply this to the principal components model that we've been developing, which looks promising but may be overfitting if we choose to include too many components.  Unfortunately PCR isn't built in to scikit-learn so creating the learning curve like above is complicated.\n",
    "\n",
    "We can apply cross-validation to PCR (similar to how we did last time) and see how cross validated scores vary with the number of components.\n",
    "\n",
    "Use the function `get_octane_observations_and_assays` to create the data and the scikit-learn `pipeline`. The pipeline should have both a `LinearRegression` and `PCA` model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X, y = \n",
    "\n",
    "regression = \n",
    "components_to_test = np.arange(1,25)\n",
    "component_scores = []\n",
    "for component in components_to_test:\n",
    "    pca = \n",
    "    pipe = Pipeline(steps=[('pca', pca), ('regression', regression)])\n",
    "    component_scores.append(cross_val_score(estimator=pipe,\n",
    "                                           X=X, y=y).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(component_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between error in prediction of octane rating and the number of components isn't simple.  This is partly because principal components aims to answer the question of how best to explain as much variance as possible in the X data in as few components as possible.  This is not the same question as how to find the fewest components that can best predict octane rating.  The latter question is closer to what PLS tries to answer.  This will be explored further soon.\n",
    "\n",
    "The plot above suggests that a good model PCR may be found with four components (being where the first local minimum appears, and because there is little substantial improvement in R^2 after that), and that additional components may be overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
